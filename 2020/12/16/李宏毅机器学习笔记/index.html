<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/themes/green/pace-theme-minimal.css">
  <script src="https://cdn.jsdelivr.net/npm/pace-js@1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"example.com","root":"/","images":"/images","scheme":"Mist","darkmode":false,"version":"8.8.1","exturl":false,"sidebar":{"position":"right","display":"hide","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":true,"transition":{"post_block":"fadeInUp","post_header":"fadeInUp","post_body":"fadeInUp","coll_header":"fadeInUp","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>
<meta name="description" content="前言 李宏毅机器学习课程从2016年秋到2021年春有很多不同的版本，讲授的内容不完全一样。每个版本都有一些变化，增添一些学术上新的发展，删去一些比较旧的技术。下面想整理一下每个学期讲授知识点的差异。">
<meta property="og:type" content="article">
<meta property="og:title" content="李宏毅机器学习笔记">
<meta property="og:url" content="http://example.com/2020/12/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/index.html">
<meta property="og:site_name" content="兴趣使然的博客">
<meta property="og:description" content="前言 李宏毅机器学习课程从2016年秋到2021年春有很多不同的版本，讲授的内容不完全一样。每个版本都有一些变化，增添一些学术上新的发展，删去一些比较旧的技术。下面想整理一下每个学期讲授知识点的差异。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202017%20Spring.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202020%20Spring.jpg">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202021%20Spring.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E5%85%83%E5%AD%A6%E4%B9%A0.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E5%AE%9D%E5%8F%AF%E6%A2%A6.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A92.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A91.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E9%9A%90%E8%97%8F%E5%9B%A0%E7%B4%A0.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%AD%A3%E5%88%99%E5%8C%96.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Estimator.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Variance.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Bias%20Vs.%20Variance.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Cross%20Validation.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_N-fold%20Cross%20Validation.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Review-Gradient%20Descent.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Stochastic%20Gradient%20Descent.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Feature%20Scaling.png">
<meta property="og:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%A0%87%E5%87%86%E5%8C%96.png">
<meta property="article:published_time" content="2020-12-16T12:38:17.000Z">
<meta property="article:modified_time" content="2021-11-06T02:47:12.888Z">
<meta property="article:author" content="想飞的鸡">
<meta property="article:tag" content="填坑中">
<meta property="article:tag" content="机器学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202017%20Spring.png">


<link rel="canonical" href="http://example.com/2020/12/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"http://example.com/2020/12/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","path":"2020/12/16/李宏毅机器学习笔记/","title":"李宏毅机器学习笔记"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>李宏毅机器学习笔记 | 兴趣使然的博客</title>
  




  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
<style type="text/css">
.spoiler {
  display: inline-flex;
}
p.spoiler {
  display: flex;
}
.spoiler a {
  pointer-events: none;
}
.spoiler-blur, .spoiler-blur > * {
  transition: text-shadow .5s ease;
}
.spoiler .spoiler-blur, .spoiler .spoiler-blur > * {
  color: rgba(0, 0, 0, 0);
  background-color: rgba(0, 0, 0, 0);
  text-shadow: 0 0 10px grey;
  cursor: pointer;
}
.spoiler .spoiler-blur:hover, .spoiler .spoiler-blur:hover > * {
  text-shadow: 0 0 5px grey;
}
.spoiler-box, .spoiler-box > * {
  transition: color .5s ease,
  background-color .5s ease;
}
.spoiler .spoiler-box, .spoiler .spoiler-box > * {
  color: black;
  background-color: black;
  text-shadow: none;
}</style></head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">兴趣使然的博客</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li>
        <li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li>
        <li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup"><div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close" role="button">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E8%AF%BE%E7%A8%8B%E7%AE%80%E4%BB%8B"><span class="nav-number">2.</span> <span class="nav-text">课程简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-number">2.1.</span> <span class="nav-text">什么是机器学习？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BD%A0%E6%83%B3%E6%89%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">2.2.</span> <span class="nav-text">你想找什么样的函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E5%91%8A%E8%AF%89%E6%9C%BA%E5%99%A8%E4%BD%A0%E6%83%B3%E6%89%BE%E4%BB%80%E4%B9%88%E6%A0%B7%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">2.3.</span> <span class="nav-text">怎么告诉机器你想找什么样的函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E6%80%8E%E4%B9%88%E6%89%BE%E5%87%BA%E4%BD%A0%E6%83%B3%E8%A6%81%E7%9A%84%E5%87%BD%E6%95%B0"><span class="nav-number">2.4.</span> <span class="nav-text">机器怎么找出你想要的函数？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E6%B2%BF%E7%A0%94%E7%A9%B6"><span class="nav-number">2.5.</span> <span class="nav-text">前沿研究</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92"><span class="nav-number">3.</span> <span class="nav-text">回归</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%9B%9E%E5%BD%92%E5%8F%AF%E4%BB%A5%E5%81%9A%E4%BB%80%E4%B9%88"><span class="nav-number">3.1.</span> <span class="nav-text">回归可以做什么？</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E9%A2%84%E6%B5%8B%E5%AE%9D%E5%8F%AF%E6%A2%A6%E8%BF%9B%E5%8C%96%E5%90%8E%E7%9A%84cp%E5%80%BC"><span class="nav-number">3.2.</span> <span class="nav-text">预测宝可梦进化后的CP值</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%80%8E%E4%B9%88%E8%A7%A3%E8%BF%99%E4%B8%AA%E9%97%AE%E9%A2%98"><span class="nav-number">3.2.1.</span> <span class="nav-text">怎么解这个问题？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%9C%E5%A6%82%E4%BD%95%E5%91%A2"><span class="nav-number">3.2.2.</span> <span class="nav-text">结果如何呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%A6%82%E4%BD%95%E5%81%9A%E5%BE%97%E6%9B%B4%E5%A5%BD%E5%91%A2"><span class="nav-number">3.2.3.</span> <span class="nav-text">如何做得更好呢？</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%BF%87%E6%8B%9F%E5%90%88"><span class="nav-number">3.2.4.</span> <span class="nav-text">过拟合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-number">3.2.5.</span> <span class="nav-text">正则化</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-number">4.</span> <span class="nav-text">基本概念</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="nav-number">5.</span> <span class="nav-text">梯度下降</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E7%BD%91%E9%A1%B5"><span class="nav-number">6.</span> <span class="nav-text">参考网页</span></a></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author site-overview-item animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">想飞的鸡</p>
  <div class="site-description" itemprop="description">文章要认认真真写鸭</div>
</div>
<div class="site-state-wrap site-overview-item animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">92</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">63</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



        </div>
      </div>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top" role="button" aria-label="返回顶部">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://example.com/2020/12/16/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="想飞的鸡">
      <meta itemprop="description" content="文章要认认真真写鸭">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="兴趣使然的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          李宏毅机器学习笔记<a href="https://github.com/xfdj/blog/edit/master/source/_posts/%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0.md" class="post-edit-link" title="编辑" rel="noopener" target="_blank"><i class="fa fa-pen-nib"></i></a>
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-12-16 20:38:17" itemprop="dateCreated datePublished" datetime="2020-12-16T20:38:17+08:00">2020-12-16</time>
    </span>
      <span class="post-meta-item">
        <span class="post-meta-item-icon">
          <i class="far fa-calendar-check"></i>
        </span>
        <span class="post-meta-item-text">更新于</span>
        <time title="修改时间：2021-11-06 10:47:12" itemprop="dateModified" datetime="2021-11-06T10:47:12+08:00">2021-11-06</time>
      </span>

  
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>8.9k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>15 分钟</span>
    </span>
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <h1 id="前言">前言</h1>
<p>李宏毅机器学习课程从2016年秋到2021年春有很多不同的版本，讲授的内容不完全一样。每个版本都有一些变化，增添一些学术上新的发展，删去一些比较旧的技术。下面想整理一下每个学期讲授知识点的差异。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202017%20Spring.png" alt="Learning Map 2017 Spring.png" /><figcaption aria-hidden="true">Learning Map 2017 Spring.png</figcaption>
</figure>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202020%20Spring.jpg" alt="Learning Map 2020 Spring.jpg" /><figcaption aria-hidden="true">Learning Map 2020 Spring.jpg</figcaption>
</figure>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Learning%20Map%202021%20Spring.png" alt="2020_12_16_李宏毅机器学习笔记_Learning Map 2021 Spring" /><figcaption aria-hidden="true">2020_12_16_李宏毅机器学习笔记_Learning Map 2021 Spring</figcaption>
</figure>
<table>
<thead>
<tr class="header">
<th>知识点</th>
<th>ML2019春</th>
<th>ML2020春</th>
<th>ML2021春</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>深度学习</td>
<td>√</td>
<td></td>
<td>√</td>
</tr>
<tr class="even">
<td>回归</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="odd">
<td>分类</td>
<td></td>
<td>√</td>
<td>√</td>
</tr>
<tr class="even">
<td>CNN</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="odd">
<td>自注意力</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr class="even">
<td>RNN</td>
<td>√</td>
<td>√</td>
<td></td>
</tr>
<tr class="odd">
<td>集成学习</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Transformer</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="odd">
<td>可解释性AI</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="even">
<td>对抗恶意攻击</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="odd">
<td>网络压缩</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="even">
<td>Seq2Seq</td>
<td>√</td>
<td>√</td>
<td></td>
</tr>
<tr class="odd">
<td>无监督学习</td>
<td>√</td>
<td>√</td>
<td></td>
</tr>
<tr class="even">
<td>异常检测</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="odd">
<td>GAN</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="even">
<td>自监督学习</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr class="odd">
<td>域适应</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr class="even">
<td>半监督学习</td>
<td>√</td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>迁移学习</td>
<td>√</td>
<td>√</td>
<td></td>
</tr>
<tr class="even">
<td>终身学习</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="odd">
<td>强化学习</td>
<td>√</td>
<td>√</td>
<td>√</td>
</tr>
<tr class="even">
<td>量子机器学习</td>
<td></td>
<td></td>
<td>√</td>
</tr>
<tr class="odd">
<td>元学习</td>
<td>√</td>
<td></td>
<td>√</td>
</tr>
</tbody>
</table>
<h1 id="课程简介">课程简介</h1>
<h2 id="什么是机器学习">什么是机器学习？</h2>
<p>机器学习就是自动找函数。</p>
<p>对于语音识别，机器学习要找的函数的输入是一段音频，输出是音频对应的文本。</p>
<p>对于图片识别，要找的函数的输入是一张图片，输出是这张图片上的对象的名称。</p>
<p>对于下围棋，要找的函数的输入是目前的棋盘的状态，输出是下一个棋子的位置。</p>
<p>对于语音助手，要找的函数的输入是用户输入的文本，输出是语音助手回答的文本。</p>
<p>明白了机器学习是在找函数之后，下一个要问的问题是：</p>
<h2 id="你想找什么样的函数">你想找什么样的函数？</h2>
<p>要弄明白你想找什么样的函数，本质上就是需要明确函数的输入是什么、输出是什么。</p>
<p>一般来说，根据你想找的函数的类别，机器学习大概可以分成几大类：</p>
<ul>
<li>回归：输出是连续的</li>
<li>分类：输出是离散的
<ul>
<li>二分类</li>
<li>多分类</li>
</ul></li>
<li>生成：产生有结构的复杂的东西。比如翻译、画图。</li>
</ul>
<p>当你弄清楚了你想找什么样的函数之后，接下来要面对的问题就是：</p>
<h2 id="怎么告诉机器你想找什么样的函数">怎么告诉机器你想找什么样的函数？</h2>
<p>根据告诉机器的方式的不同，机器学习可以大概分为几大类：</p>
<ul>
<li>监督学习：给机器训练资料，这些训练资料是指带标签的数据。有了这些训练资料，机器就可以评估一个函数的好坏，方式是使用损失函数Loss来表示函数有多不好。Loss越小越好。有了评估一个函数好坏的方式之后，机器就可以自动找出Loss最低的函数。</li>
<li>无监督学习：给机器训练数据，但是这些数据不带标签。</li>
<li>强化学习：给机器一个环境，让机器在环境里执行一些动作，环境会反馈给机器一些奖励信息，这样机器可以找到如何获取最多的奖励。</li>
</ul>
<p>在你告诉了机器你想找什么样的函数之后，下一个问题就是：</p>
<h2 id="机器怎么找出你想要的函数">机器怎么找出你想要的函数？</h2>
<p>机器找函数的步骤可以概括为以下两步：</p>
<ol type="1">
<li>给定函数的寻找范围，比如说函数的结构是线性结构还是网络结构。</li>
<li>机器在给定范围内寻找好的函数，函数寻找的方法是梯度下降。</li>
</ol>
<h2 id="前沿研究">前沿研究</h2>
<ul>
<li><p>可解释性的AI：指的是AI不仅能告诉你它的选择，还能告诉你它这么选择的理由。</p></li>
<li><p>对抗攻击（Adversarial Attack）：如果怀着恶意攻击AI系统会发生什么？对抗攻击通过制造刻意的数据（人类无法分辨这些数据的区别）来使AI得出一个错误的结果。</p></li>
<li><p>网络压缩：把规模很大的网络缩小（减少参数）。</p></li>
<li><p>异常检测：如果看到不知道怎么判断的东西，机器怎么能知道自己不知道呢？（机器如何发现自己的无知）</p></li>
<li><p>迁移学习：当训练集和测试集有不同的分布时，怎么让机器学到一些东西。</p></li>
<li><p>元学习（Meta Learning）：现有的学习算法是人类设计的，这些算法都不太聪明，需要机器进行大量的训练（勤奋不懈却天资不佳），能不能让机器更聪明一点？元学习尝试让机器学习如何学习，让机器自己发明学习算法。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E5%85%83%E5%AD%A6%E4%B9%A0.png" alt="元学习.png" /><figcaption aria-hidden="true">元学习.png</figcaption>
</figure></li>
<li><p>终身学习：让机器持续学习，增量学习，永远不停止学习，期望机器能够越来越强。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E7%BB%88%E8%BA%AB%E5%AD%A6%E4%B9%A0.png" alt="终身学习.png" /><figcaption aria-hidden="true">终身学习.png</figcaption>
</figure></li>
</ul>
<h1 id="回归">回归</h1>
<h2 id="回归可以做什么">回归可以做什么？</h2>
<ul>
<li>股票预测系统：输入之前的股票信息，输出明天的股票价格。</li>
<li>自动驾驶：输入路面信息，输出方向盘的角度。</li>
<li>推荐系统：输入顾客和商品的信息，输出购买的可能性。</li>
</ul>
<h2 id="预测宝可梦进化后的cp值">预测宝可梦进化后的CP值</h2>
<p>输入是宝可梦的各种数据（比如当前CP值、HP值、体重、身高），输出进化后的CP值。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E5%AE%9D%E5%8F%AF%E6%A2%A6.png" alt="宝可梦.png" /><figcaption aria-hidden="true">宝可梦.png</figcaption>
</figure>
<h3 id="怎么解这个问题">怎么解这个问题？</h3>
<ol type="1">
<li><p>找一个模型（函数的集合）：模型中有一堆结构相同的函数，它们只有参数的取值的区别。</p>
<p>这里假设找的模型是线性模型： <span class="math display">\[
y = b + w \cdot x_{CP}
\]</span> <span class="math inline">\(b\)</span>和<span class="math inline">\(w\)</span>是参数，可以取任何的值。</p>
<p>线性模型的一般形式是： <span class="math display">\[
y = b + \sum w_i x_i
\]</span></p></li>
<li><p>定义函数的好坏：收集很多样本，用上标表示样本的编号，用下标表示特征的编号。</p>
<p>比如说： <span class="math display">\[
\begin{matrix}
(x^1, \hat y^1) \\
(x^2, \hat y^2) \\
\vdots \\
(x^{10}, \hat y^{10})
\end{matrix}
\]</span> 定义损失函数L：输入是一个函数，输出是一个数值，表示这个函数有多不好。 <span class="math display">\[
\begin{eqnarray}
L(f) &amp; = &amp; L(w, b) \\
 &amp; = &amp; \sum^{10}_{n=1}\left(\hat y^n - \left(b + w \cdot x^n_{CP}\right)\right)^2
\end{eqnarray}
\]</span></p></li>
<li><p>挑选最好的函数：找一个函数使得它的损失函数最小。 <span class="math display">\[
\begin{eqnarray}
f^* &amp; = &amp; \arg \min_f L(f) \\
w^*, b^* &amp; = &amp; \arg \min_{w, b} L(w, b) \\
 &amp; = &amp; \arg \min_{w, b} \sum^{10}_{n=1}\left(\hat y^n - \left(b + w \cdot x^n_{CP}\right)\right)^2
\end{eqnarray}
\]</span> 梯度下降是怎么做的呢？</p>
<p>首先，随机选取一个初始的参数值<span class="math inline">\(w^0, b^0\)</span>。</p>
<p>在选取位置计算损失函数对参数的梯度。 <span class="math display">\[
\left.\frac{\partial L}{\partial w}\right|_{w=w^0, b=b^0}, \left.\frac{\partial L}{\partial b}\right|_{w=w^0, b=b^0} \\
\nabla L = \left[\begin{matrix}
\frac{\partial L}{\partial w} \\
\frac{\partial L}{\partial b}
\end{matrix}\right]_{gradient}
\]</span> 从梯度得知参数的更新方向。</p>
<p>要更新多少呢？更新步长取决于：1. 梯度 2. 学习率。 <span class="math display">\[
w^1 \leftarrow w^0 - \eta \left.\frac{\partial L}{\partial w}\right|_{w=w^0, b=b^0}, b^1 \leftarrow b^0 - \eta \left.\frac{\partial L}{\partial b}\right|_{w=w^0, b=b^0}
\]</span> 重复上述步骤，直到收敛到局部最小值。</p></li>
</ol>
<p>梯度下降得到的结果是看人品的，有可能只收敛到局部最小值，幸运的是，线性回归的损失函数是凸函数，没有局部最小值。</p>
<h3 id="结果如何呢">结果如何呢？</h3>
<p>用梯度下降求出结果之后，我们真正关心的是模型在新数据（测试集）下的误差。</p>
<p>一般来说，新数据下的误差比训练数据的误差要大。</p>
<h3 id="如何做得更好呢">如何做得更好呢？</h3>
<p>选择另外的模型，比如二次模型。 <span class="math display">\[
y = b + w_1 \cdot x_{CP} + w_2 \cdot (x_{CP})^2
\]</span> 或者三次、四次等其它模型。</p>
<p>试了5个模型（一次到五次）之后，发现在训练集上的误差越来越小，但是在测试集上的误差先变小再变大。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A92.png" alt="模型选择2.png" /><figcaption aria-hidden="true">模型选择2.png</figcaption>
</figure>
<h3 id="过拟合">过拟合</h3>
<p>复杂的模型不一定在测试集上有好的结果，这被叫做<strong>过拟合</strong>。</p>
<p>所以要选择合适的模型。</p>
<p>为什么模型次数越高，在训练集上的误差就越小呢？因为高次模型的空间总是包含低次模型的空间。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%A8%A1%E5%9E%8B%E9%80%89%E6%8B%A91.png" alt="模型选择1.png" /><figcaption aria-hidden="true">模型选择1.png</figcaption>
</figure>
<p>有时候当数据量不够的时候是无法看出数据之间的规律的。当收集到足够多的数据的时候，可能可以发现一些隐藏的影响因素。对于宝可梦来说这个隐藏因素可能是物种。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E9%9A%90%E8%97%8F%E5%9B%A0%E7%B4%A0.png" alt="隐藏因素.png" /><figcaption aria-hidden="true">隐藏因素.png</figcaption>
</figure>
<p>当数据量足够多的时候，如果发现了一些隐藏的因素在影响，可以回过头重新设计模型。</p>
<p>为了匹配模型，还可以对样本的特征进行特征转换。特征转换有时候可以让模型保持简单（比如保持线性模型）。</p>
<p>如果重新设计了一个很复杂的模型，最后发现还是过拟合了，有没有别的办法来处理？正则化。</p>
<h3 id="正则化">正则化</h3>
<p>正则化就是重新设计损失函数，加入正则化项，这会导致机器倾向于选择更平滑的、参数更小的函数。 <span class="math display">\[
y = b + \sum w_i x_i \\
L = \sum_n \left(\hat y^n - \left(b + \sum w_i x_i\right)\right)^2 + \lambda \sum (w_i)^2
\]</span> 为什么参数更小的函数更平滑？因为参数越小对输入的波动越不敏感。</p>
<p>为什么我们喜欢平滑的函数？平滑的函数受到噪声的影响比较小。</p>
<p><span class="math inline">\(\lambda\)</span>也要取合适的值。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%AD%A3%E5%88%99%E5%8C%96.png" alt="正则化.png" /><figcaption aria-hidden="true">正则化.png</figcaption>
</figure>
<p>正则化项可能会使得训练结果变差，但是可能会使得测试结果变好。不过太平滑也不行。</p>
<p>多平滑合适呢？要调整<span class="math inline">\(\lambda\)</span>的值来找到合适的模型。</p>
<p>正则化项是不需要考虑偏移量（bias）的，因为偏移量不影响函数的平滑程度。</p>
<h1 id="基本概念">基本概念</h1>
<p>模型越复杂，在测试集上的表现不一定更好，意思是误差不一定越低。</p>
<p>那么误差都来自什么地方呢？偏差（bias）和方差（variance）。</p>
<p>为什么要了解误差都来自什么地方呢？因为如果你知道误差的来源，就可以挑选适当的方法来改进你的模型。</p>
<p>假设说我们要对宝可梦进化后的CP值做一个估测，也就是说我们要找一个估计量（estimator），一个函数，输入一个宝可梦，可以输出它的CP值，游戏里面实际使用的函数<span class="math inline">\(\hat f\)</span>的具体细节我们是不知道的。<span class="math inline">\(\hat f\)</span>表示理想模型，<span class="math inline">\(f^*\)</span>表示我们从训练数据中学习到的模型，<span class="math inline">\(f^*\)</span>是<span class="math inline">\(\hat f\)</span>的估计值。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Estimator.png" alt="Estimator.png" /><figcaption aria-hidden="true">Estimator.png</figcaption>
</figure>
<p>我们找到的<span class="math inline">\(f^*\)</span>一般是不等于<span class="math inline">\(\hat f\)</span>的，<span class="math inline">\(f^*\)</span>会和<span class="math inline">\(\hat f\)</span>有一段“距离”。这段距离可能来自于偏差，也有可能来自于方差。一个估计量的偏差和方差指的是什么呢？</p>
<p>先来看一个概率论里面的例子。考虑一个随机变量<span class="math inline">\(x\)</span>，<span class="math inline">\(x\)</span>的期望值是<span class="math inline">\(\mu\)</span>，方差是<span class="math inline">\(\sigma^2\)</span>。现在来估计<span class="math inline">\(x\)</span>的期望值<span class="math inline">\(\mu\)</span>，方法是采<span class="math inline">\(N\)</span>个样本<span class="math inline">\(x_1, x_2, \dots, x_N\)</span>，令<span class="math inline">\(\bar x\)</span>等于<span class="math inline">\(x_i\)</span>的均值</p>
<p><span class="math display">\[
\bar x=\frac{1}{N}\sum_{i=1}^N x_i
\]</span></p>
<p>容易发现<span class="math inline">\(\bar x \neq \mu\)</span>，但是<span class="math inline">\(\bar x\)</span>的期望值<span class="math inline">\(E[\bar x]\)</span>等于<span class="math inline">\(\mu\)</span>。 <span class="math display">\[
E[\bar x]=E\left[\frac{1}{N}\sum_{i=1}^N x_i\right]=\frac{1}{N}\sum_{i=1}^N E[x]=\mu
\]</span> 这时我们称<span class="math inline">\(\mu\)</span>的估计<span class="math inline">\(\bar x\)</span>是无偏的。</p>
<p>这个时候<span class="math inline">\(\bar x\)</span>的方差 <span class="math display">\[
Var[\bar x] = \frac{\sigma^2}{N}
\]</span> 当<span class="math inline">\(N\)</span>越大的时候<span class="math inline">\(\bar x\)</span>的方差越小。</p>
<p>如果要估计<span class="math inline">\(\sigma^2\)</span>怎么办呢？一样可以采取刚才的方式。 <span class="math display">\[
s^2=\frac 1 N \sum_{i=1}^{N}(x_i-\bar x)^2
\]</span> 这个时候<span class="math inline">\(s^2\)</span>是对<span class="math inline">\(\sigma^2\)</span>的估计，但是这个估计是一个有偏估计。因为<span class="math inline">\(s^2\)</span>的期望不等于<span class="math inline">\(\sigma^2\)</span>。 <span class="math display">\[
E[s^2]=\frac{N-1}{N}\sigma^2\neq\sigma^2
\]</span> 随着<span class="math inline">\(N\)</span>的增大，<span class="math inline">\(s^2\)</span>和<span class="math inline">\(\sigma^2\)</span>的差距会逐渐变小。</p>
<p>回到<span class="math inline">\(f^*\)</span>的问题中来。回归中的误差也来自于这两件事，偏差和方差。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE.png" alt="偏差和方差.png" /><figcaption aria-hidden="true">偏差和方差.png</figcaption>
</figure>
<p>虽然<span class="math inline">\(f^*\)</span>只有一个，但是也是隐含了偏差和方差在里面，这个偏差和方差是训练样本带来的。假设有很多组不同的训练样本，就可以训练出很多不同的<span class="math inline">\(f^*\)</span>。使用不同模型得到的<span class="math inline">\(f^*\)</span>会有一些不同。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Variance.png" alt="Variance.png" /><figcaption aria-hidden="true">Variance.png</figcaption>
</figure>
<p>我们先来看方差。对于比较简单的模型来说，方差会比较小，而对于复杂的模型来说，方差会比较大。</p>
<p>为什么比较复杂的模型散布得比较开呢？因为比较简单的模型不容易受到样本数据的影响。</p>
<p>当我们使用复杂的模型的时候，虽然每一次的方差会很大，但是所有模型的平均值会很接近最终的理想模型。</p>
<p>当模型越来越复杂的时候，偏差会减小，方差会增大。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Bias%20Vs.%20Variance.png" alt="Bias Vs. Variance.png" /><figcaption aria-hidden="true">Bias Vs. Variance.png</figcaption>
</figure>
<p>如果误差来自于方差的话，那么这种情况就是过拟合；如果误差来自于偏差的话，那么这种情况就是欠拟合。</p>
<p>如何处理偏差大的情况呢？</p>
<p>首先如何知道自己是偏差大的情况？如果你的模型没有办法很好地你和训练数据的话，你的偏差就很大，这种时候就是欠拟合。但是如果你的模型训练数据拟合得很好，但是测试数据误差很大，那么你很可能方差很大，这种时候就是过拟合。</p>
<p>当你知道自己的偏差很大的时候，你就需要重新设计你的模型，因为理想模型可能根本不在你的模型范围里面。可以做的事情包括：在输入中增加更多的特征，或者使用更复杂的模型。偏差很大的时候就没有必要去收集更多的数据了。</p>
<p>如何处理方差大的情况呢？</p>
<p>当你的模型方差很大的时候，其中一个办法就是增加你的数据。但是不见得现实。还有一种方法是自己制造假的数据，根据自己对问题的理解。正则化也是一种方法，但是有可能会使偏差变大，所以要调节正则化的权重，在平滑度和小偏差之间达到一种平衡。</p>
<p>在模型的选择过程中，经常需要在偏差和方差之间找一个平衡。有时候在你手上的测试集表现很好的模型，不一定在实际的测试集上表现很好，所以不要一味选择在你手上的测试集表现最好的模型。</p>
<p>正确的做法是交叉验证（Cross Validation）。把训练集再细分成训练集和验证集，在新的训练集上训练模型，然后在验证集上选择模型，选择在验证集表现最好的模型之后，用全部的训练集再训练最好的模型，然后在测试集上验证。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Cross%20Validation.png" alt="Cross Validation.png" /><figcaption aria-hidden="true">Cross Validation.png</figcaption>
</figure>
<p>如果担心分出来的验证集也有偏差怎么办？可以使用N折交叉验证（N-fold Cross Validation）。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_N-fold%20Cross%20Validation.png" alt="N-fold Cross Validation.png" /><figcaption aria-hidden="true">N-fold Cross Validation.png</figcaption>
</figure>
<h1 id="梯度下降">梯度下降</h1>
<p>来复习一下之前提到过的梯度下降。我们需要解决如下的优化问题。 <span class="math display">\[
\theta^*=\arg\min_\theta L(\theta)
\]</span> 损失函数<span class="math inline">\(L(\theta)\)</span>可以看作函数的函数，它的输入是一个函数，或者是可以代表函数的参数。它可以评价一个函数“有多坏”。我们需要找到使得<span class="math inline">\(L(\theta)\)</span>最小的参数<span class="math inline">\(\theta^*\)</span>。下面来回忆一下梯度下降是怎么做的。</p>
<p>假设<span class="math inline">\(\theta\)</span>包含两个分量<span class="math inline">\(\theta_1, \theta_2\)</span>。</p>
<p>首先随机初始化<span class="math inline">\(\theta\)</span>的值。 <span class="math display">\[
\theta^0=\begin{bmatrix}
\theta^0_1  \\
\theta^0_2
\end{bmatrix}
\]</span> 之后如下更新： <span class="math display">\[
\begin{bmatrix}
\theta^1_1 \\
\theta^1_2
\end{bmatrix}
=
\begin{bmatrix}
\theta^0_1 \\
\theta^0_2
\end{bmatrix}
- \eta
\begin{bmatrix}
\frac{\partial L(\theta^0)}{\partial \theta_1} \\
\frac{\partial L(\theta^0)}{\partial \theta_2}
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[
\begin{bmatrix}
\theta^2_1 \\
\theta^2_2
\end{bmatrix}
=
\begin{bmatrix}
\theta^1_1 \\
\theta^1_2
\end{bmatrix}
- \eta
\begin{bmatrix}
\frac{\partial L(\theta^1)}{\partial \theta_1} \\
\frac{\partial L(\theta^1)}{\partial \theta_2}
\end{bmatrix}
\]</span></p>
<p>其中偏导数的部分可以写成 <span class="math display">\[
\nabla L(\theta)=\begin{bmatrix}
\frac{\partial L(\theta)}{\partial \theta_1} \\
\frac{\partial L(\theta)}{\partial \theta_2}
\end{bmatrix}
\]</span> 这样更新部分可以写成 <span class="math display">\[
\theta^{t+1} = \theta^t - \eta \nabla L(\theta^t)
\]</span> 可以把梯度方向理解成损失函数的等高线的法线方向。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Review-Gradient%20Descent.png" alt="Review-Gradient Descent.png" /><figcaption aria-hidden="true">Review-Gradient Descent.png</figcaption>
</figure>
<p>梯度下降有几个小提示。</p>
<ul>
<li><p>小心调整学习率<span class="math inline">\(\eta\)</span>。如果学习率太大，损失函数可能会越更新越大，如果学习率太小，损失函数可能会下降得很慢。只有刚刚好的学习率，才有可能得到一个好的结果。所以应该要把损失函数的变化图画出来，根据前几步的图像调整学习率，要确定损失函数能够稳定地下降。</p>
<p>有一些自动的办法可以帮我们调整学习率。最简单的一个想法是学习率会随着参数的更新越来越小。因为刚开始离最低点比较远，步子要迈得大一点，等到离最低点很近的时候，步子要迈小一点。</p>
<p>但是这样子还不够，还有一种想法是给每一个参数不同的学习率。这其中一个最简单最容易实现的叫做Adagrad。它的思想是将每一个参数的学习率除以该参数之前所有微分的均方根（RMS）。</p>
<p>假设一般的梯度下降是 <span class="math display">\[
\theta^{t+1}=\theta^t-\eta^t\frac{\partial L(\theta^t)}{\partial \theta^t}
\]</span> 这里假设<span class="math inline">\(\eta^t=\frac{\eta}{\sqrt{t+1}}\)</span></p>
<p>Adagard则是 <span class="math display">\[
\theta^{t+1}=\theta^t-\frac{\eta^t}{\sigma^t}\frac{\partial L(\theta^t)}{\partial \theta^t}
\]</span> 这里的<span class="math inline">\(\sigma^t\)</span>是<span class="math inline">\(\theta\)</span>之前微分的均方根。这个值对每一个参数都是不一样的。 <span class="math display">\[
\sigma^t=\sqrt{\frac{1}{t+1}\sum^t_{i=0}\left(\frac{\partial L(\theta^i)}{\partial \theta^i}\right)^2}
\]</span> 化简一下可以变成 <span class="math display">\[
\theta^{t+1}=\theta^t-\frac{\eta}{\sqrt{\sum^t_{i=1}\left(\frac{\partial L(\theta^i)}{\partial \theta^i}\right)^2}}\frac{\partial L(\theta^i)}{\partial \theta^i}
\]</span> 当然自适应地调整学习率有一系列的方法，如果没有什么需求，可以使用Adam。</p></li>
<li><p>最好的步长和一次微分成正比，和二次微分成反比。Adagrad神奇的地方在于它的步长正好正比于一次微分，反比于二次微分。分母的一坨东西想要代表的是二次微分。Adagrad想做的事情是在不直接计算出二次微分的情况下，能不能想办法估计一下二次微分的大小，并且Adagrad只需要一次微分的值。所以这个分母的值可以理解成一种“反差”的大小，如果二次微分比较小，那么在局部最低点附近的一次微分的值也比较小，那采样出来的几个一次微分的值求平方和开根号也比较小，反之亦然，所以分母从某种程度上代表了二次微分值的大小。</p></li>
<li><p>随机梯度下降。修改损失函数的表达，从 <span class="math display">\[
L=\sum_n\left(\hat y^n - \left( b+\sum w_i x^n_i\right)\right)^2
\]</span> 到随机取一个样本<span class="math inline">\(x^n\)</span> <span class="math display">\[
L^n=\left(\hat y^n - \left( b+\sum w_i x^n_i\right)\right)^2
\]</span> 从算所有训练数据的损失到只算一个训练数据的损失。</p>
<p>也就是从 <span class="math display">\[
\theta^{t+1} = \theta^t - \eta \nabla L(\theta^t)
\]</span> 到 <span class="math display">\[
\theta^{t+1} = \theta^t - \eta \nabla L^n(\theta^t)
\]</span> <img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Stochastic%20Gradient%20Descent.png" alt="Stochastic Gradient Descent.png" /></p>
<p>随机梯度下降好在哪里呢？原始的梯度下降走的方向很稳，但是走一步的计算量很大，而随机梯度下降虽然一步的方向不是很正确，但是计算速度快，走完一个批次之后的结果可能比原始的梯度下降要好。</p></li>
<li><p>特征缩放。主要思想是希望不同的特征，有相同的范围。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_Feature%20Scaling.png" alt="Feature Scaling.png" /><figcaption aria-hidden="true">Feature Scaling.png</figcaption>
</figure>
<p>为什么要这么做呢？感觉其实这和给每个参数不同的学习率有异曲同工之妙。</p>
<p>其中一种特征缩放的方式就是标准化。让所有特征的均值都是0，方差都是1。</p>
<figure>
<img src="https://xfdjblog-1258179712.cos.ap-guangzhou.myqcloud.com/2020_12_16_%E6%9D%8E%E5%AE%8F%E6%AF%85%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0_%E6%A0%87%E5%87%86%E5%8C%96.png" alt="标准化.png" /><figcaption aria-hidden="true">标准化.png</figcaption>
</figure></li>
</ul>
<p>梯度下降有效的根本原因是因为泰勒展开。所以理论上，根据泰勒展开，可以把损失函数用二次微分近似，但是这样产生的运算（Hessian matrix等等）是无法承受的。</p>
<p>多变量泰勒展开的近似 <span class="math display">\[
\begin{eqnarray}
h(x, y) &amp; = &amp; h(x_0, y_0)+\frac{\partial h(x_0, y_0)}{\partial x}(x-x_0)+\frac{\partial h(x_0, y_0)}{\partial y}(y-y_0) + ...\\
&amp; \approx &amp; h(x_0, y_0)+\frac{\partial h(x_0, y_0)}{\partial x}(x-x_0)+\frac{\partial h(x_0, y_0)}{\partial y}(y-y_0)
\end{eqnarray}
\]</span> 梯度下降的一个弊端是它可能会卡在局部最优解或者鞍点等微分是0的地方，也有可能会在一个类似高原的地方更新缓慢。</p>
<h1 id="参考网页">参考网页</h1>
<ul>
<li><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av94519857">李宏毅2020机器学习深度学习(完整版)国语_哔哩哔哩 (゜-゜)つロ 干杯~-bilibili</a></li>
<li><a target="_blank" rel="noopener" href="https://speech.ee.ntu.edu.tw/~hylee/">Hung-yi Lee (李宏毅)</a></li>
</ul>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%A1%AB%E5%9D%91%E4%B8%AD/" rel="tag"><i class="fa fa-tag"></i> 填坑中</a>
              <a href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/" rel="tag"><i class="fa fa-tag"></i> 机器学习</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2020/11/21/%E3%80%90%E8%BD%AC%E3%80%91DFL%E5%AE%98%E6%96%B9%E4%BD%BF%E7%94%A8%E8%AF%B4%E6%98%8E/" rel="prev" title="【转】DFL官方使用说明">
                  <i class="fa fa-chevron-left"></i> 【转】DFL官方使用说明
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2020/12/18/mdx%E3%80%81mdd%E4%B8%8Ecss%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/" rel="next" title="mdx、mdd与css之间的关系">
                  mdx、mdd与css之间的关系 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">


<div class="copyright">
  &copy; 
  <span itemprop="copyrightYear">2021</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">想飞的鸡</span>
</div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
      <span>站点总字数：</span>
    <span title="站点总字数">344k</span>
  </span>
</div>

    </div>
  </footer>

  
  <script src="https://cdn.jsdelivr.net/npm/animejs@3.2.1/lib/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/@next-theme/pjax@0.5.0/pjax.min.js" integrity="sha256-3NkoLDrmHLTYj7csHIZSr0MHAFTXth7Ua/DDt4MRUAg=" crossorigin="anonymous"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>
  <script src="https://cdn.bootcdn.net/ajax/libs/pangu/4.0.7/pangu.min.js"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  
<script src="https://cdn.jsdelivr.net/npm/hexo-generator-searchdb@1.4.0/dist/search.js" integrity="sha256-vXZMYLEqsROAXkEw93GGIvaB2ab+QW6w3+1ahD9nXXA=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>


  <script class="next-config" data-name="mermaid" type="application/json">{"enable":true,"theme":{"light":"default","dark":"dark"},"js":{"url":"https://cdn.bootcdn.net/ajax/libs/mermaid/8.11.2/mermaid.min.js"}}</script>
  <script src="/js/third-party/tags/mermaid.js"></script>

  <script src="/js/third-party/fancybox.js"></script>

  <script src="/js/third-party/pace.js"></script>

  




  

  <script class="next-config" data-name="enableMath" type="application/json">true</script><script class="next-config" data-name="mathjax" type="application/json">{"enable":true,"tags":"none","js":{"url":"https://cdn.bootcdn.net/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.min.js"}}</script>
<script src="/js/third-party/math/mathjax.js"></script>



<script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"pluginRootPath":"live2dw/","pluginJsPath":"lib/","pluginModelPath":"assets/","tagMode":false,"debug":false,"model":{"scale":1,"jsonPath":"/live2dw/assets/koharu.model.json"},"display":{"superSample":2,"hOffset":0,"vOffset":0,"position":"left","width":200,"height":300},"mobile":{"show":false,"scale":0.2},"react":{"opacity":0.5},"log":false});</script></body>
</html>
